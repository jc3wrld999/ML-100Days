{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f0e15c",
   "metadata": {},
   "source": [
    "## 문서 분류 모델 훑어보기\n",
    "- 문서 분류란 문서가 주어졌을 때 해당 문서의 범주를 분류하는 과제이다.\n",
    "- 뉴스를 입력하고 정치, 경제, 연예 등 범주를 맞히거나, 영화 리뷰가 긍정/부정 등 어떤 극성을 가지는지 분류\n",
    "\n",
    "- 이번에 사용할 데이터는 네이버 영화 리뷰 말뭉치(NSMC)\n",
    "- 문서 분류 모델은 영화 리뷰 문장을 입력으로 하고 해당 문장이 속한 극성의 확률을 출력한다.\n",
    "- 문서 분류 모델의 출력은 확률값이므로 적당한 후처리 과정을 거쳐 긍정, 부정처럼 사람이 보기에 좋은 형태로 가공해준다.\n",
    "\n",
    "### 모델 구조\n",
    "- 여기서 사용하는 문서분류 모델은 입력 문장을 토큰화한 뒤 \n",
    "- 문장 시작과 끝을 알리는 스페셜 토큰 CLS와 SEP를 각각 원래 토큰 시퀀스 앞뒤에 붙인다. \n",
    "- 이를 BERT 모델에 입력하고 문장 수준의 벡터(pooler_output)를 뽑는다.\n",
    "- 이 벡터에 작은 추가 모듈을 덧붙여 모델 전체의 출력이 [해당 문장이 긍정일 확률, 해당 문장이 부정일 확률] 형태가 되도록한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f8e9b",
   "metadata": {},
   "source": [
    "### 1. 모델 환경 설정\n",
    "- kcbert-base 모델을 NSMC 데이터로 파인튜닝해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56a14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n",
    "args = ClassificationTrainArguments(\n",
    "    pretrained_model_name=\"beomi/kcbert-base\",\n",
    "    downstream_corpus_name=\"nsmc\",\n",
    "    downstream_model_dir=\"nlpbook/checkpoint-doccls\",\n",
    "    batch_size=32 if torch.cuda.is_available() else 4,\n",
    "    learning_rate=5e-5,\n",
    "    max_seq_length=128,\n",
    "    epochs=3,\n",
    "    tpu_cores=0 if torch.cuda.is_available() else 8,\n",
    "    seed=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc51a59",
   "metadata": {},
   "source": [
    "TrainArguments의 매개변수\n",
    "- pretrained_model_name: 프리트레인 마친 언어 모델의 이름(단 해당 모델은 허깅페이스 모델 허브에 등록되어 있어야함)\n",
    "- downstream_corpus_name: 다운스트림 데이터의 이름\n",
    "- downstream_model_root_dir: 다운스트림 데이터를 내려받을 위치. 입력하지 않으면 /content/Korpora에 저장됨\n",
    "- downstream_model_dir: 파인튜닝된 모델의 체크포인트가 저장될 위치.\n",
    "- batch_size: 배치 크기. 하드웨어 가속기로 GPU를 선택(torch.cuda.is_available()==True)했다면 32, TPU(torch.cuda.is_available()==False)라면 4/ 코랩환경에서 TPU는 보통 8개 코어가 할당되는데 batch_size는 코어별로 적용되는 배치 크기이므로 이렇게 설정해둔다.\n",
    "- learning_rate: 러닝 레이트(보폭). 1회 스텝에서 모델을 얼마나 업데이트할지에 관한 크기를 가리킨다.\n",
    "- max_seq_length: 토큰 기준 입력 문장 최대 길이. 이보다 긴 문장은 max_seq_length로 자르고 짧은 문장은 max_seq_length가 되도록 스페셜 토큰([PAD])를 붙여줌\n",
    "- epochs: 학습 에포크 수. 3이라면 학습 데이터 전체를 3회 반복\n",
    "- tpu_cores: TPU 코어수. 하드웨어 가속기로 GPU를 선택했다면 0,TPU라면 8\n",
    "- seed: 랜덤 시드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc78bb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set seed: 7\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp import nlpbook\n",
    "nlpbook.set_seed(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc6842f",
   "metadata": {},
   "source": [
    "- args에 지정된 시드로 고정하는 역할\n",
    "- 같은 시드를 사용하면 컴퓨터는 계속 같은 패턴의 난수를 생성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fd36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpbook.set_logger(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8241d8",
   "metadata": {},
   "source": [
    "### 2. 말뭉치 내려받기\n",
    "- NSMC 데이터를 내려받자.\n",
    "- kopora라이브러리로 데이터를 내려받고 corpus_name(nsmc)에 해당하는 말뭉치르 root_dir아래에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878c981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:00, 19.4MB/s]                                                            \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 17.0MB/s]                                                             \n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch(\n",
    "    corpus_name=args.downstream_corpus_name,\n",
    "    root_dir=args.downstream_corpus_root_dir,\n",
    "    force_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f14dda",
   "metadata": {},
   "source": [
    "### 3. 토크나이저 준비하기\n",
    "- 여기서 다루는 데이터의 기본단위는 텍스트 형태의 문장이다.\n",
    "- 토큰화란 문장을 토큰 시퀀스로 분절하는 과정을 가리킨다.\n",
    "- 여기서 사용하는 모델은 자연어 문장을 분절한 토큰 시퀀스를 입력으로 받는다.\n",
    "- kcbert-base모델이 사용하는 토크나이저를 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475c7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f69d1",
   "metadata": {},
   "source": [
    "### 4. 데이터 전처리하기\n",
    "- 딥러닝 모델을 학습하려면 학습 데이터를 배치 단위로 계속 모델에 공급해 주어야 한다.\n",
    "- 파이토치에선 이 역할을 데이터 로더가 수행한다.\n",
    "- 데이터 로더는 데이터 셋이 보유하고 있는 인스턴스를 배치 크기만큼 뽑아 자료형, 데이터 길이 등 정해진 형식에 맞춰 배치를 만들어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e7fd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset\n",
    "corpus = NsmcCorpus()\n",
    "train_dataset = ClassificationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829c8fb",
   "metadata": {},
   "source": [
    "ClassificationDataset 의 역할\n",
    "- NsmcCorpus랑 tokenizer를 품고있음\n",
    "- NsmcCorpus는 CSV 파일 형식의 NSMC 데이터를 문장(영화리뷰)과 레이블(긍정, 부정)로 읽어 들임\n",
    "- NsmcCorpus는 ClassificationDataset이 요구하면 이 문장과 레이블을 ClassificationDataset에 제공함\n",
    "- ClassificationDataset는 NsmcCorpus가 넘겨준 문장과 레이블 각각을 tokenizer를 활용해 모델이 학습 할 수 있는 형태(ClassificationFeatures)로 가공함\n",
    "- ClassificationFeatures라는 자료형에는 4가지 정보가 있음\n",
    "    - input_ids: 인덱스로 변환된 토큰 시퀀스(list(int))\n",
    "    - attention_mask: 토큰(1)인지 패딩(0)인지(list(int))\n",
    "    - token_type_ids: 세그먼트 정보(list(int))\n",
    "    - label: 정수로 바뀐 레이블 정보(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81cda18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationFeatures(input_ids=[2, 2170, 832, 5045, 17, 17, 7992, 29734, 4040, 10720, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211d820",
   "metadata": {},
   "source": [
    "- NsmcCorpus가 넘겨준 0번 데이터(text:아 더빙.. 진짜 짜증나네요 목소리, label: 0)가 ClassificationFeatures라는 0번 인스턴스로 변환된것\n",
    "- train_dataset[0] 처럼 ClassificationDataset이 가지고 있는 모든 인스턴스는 인덱스로 접근할 수 있음\n",
    "- train_dataset[0].input_ids, train_dataset[0].attention_mask, train_dataset[0].token_type_ids의 길이가 128인 이유는 토큰 기준 최대 길이(max_seq_length)를 코드의 args에서 128로 설정해 두었기 때문\n",
    "- 세그먼트 정보(token_type_ids)를 입력하는 건 BERT 모델의 특징이다.\n",
    "- BERT의 프리트레인 과제는 '빈칸 맞히기'외에 '이어진 문서인지 맞히기'도 있다.\n",
    "- 문서 2개를 입력하고 2개의 문서가 이어진 것인지 아닌지를 이진 분류하는 과정에서 프리트레인을 수행한다\n",
    "- BERT의 세그먼트 정보는 첫번째 문서에 해당하는 토큰 시퀀스가 0, 두번째 문서의 토큰 시퀀스가 1이 되도록 만든다.\n",
    "- 하지만 우리는 영화리뷰 문서 하나를 입력하고 그 문서의 극성을 분류하는 과제를 수행 중이다.\n",
    "- 따라서 이 실습에서 모든 인스턴스의 세그먼트 정보는 0으로 넣는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb9c75",
   "metadata": {},
   "source": [
    "### 학습 데이터 로더 구축\n",
    "- 데이터 로더는 ClassificationDataset 클래스가 들고 있는 전체 인스턴스 가운데 배치 크기(args의 batch_size)만큼을 뽑아 배치 형태로 가공(nlpbook.data_collator)하는 역할을 수행\n",
    "- sampler: 샘플링 방식을 정의/ 여기서 데이터로더는 배치를 만들 때 ClassificationDataset이 들고 있는 전체 인스턴스 가운데 batch_size 개수만큼을 비복원(replacement=False) 랜덤추출(Random Sampler)한다.\n",
    "- collate_fn: 이렇게 뽑은 인스턴스들을 배치로 만드는 역할/ nlpbook.data_collator는 같은 배치에서 인스턴스가 여럿일 때 이를 input_ids, attention_mask 등 종류별로 모으고 파이토치가 요구하는 자료형인 텐서 형태로 바꾸는 역할을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bd8ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(train_dataset, replacement=False),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5a295",
   "metadata": {},
   "source": [
    "### 테스트 데이터 로더 구축\n",
    "- 평가용 데이터 로더는 SequentialSampler를 사용\n",
    "- SequentialSampler는 인스턴스를 batch_size만큼 순서대로 추출하는 역할을 함\n",
    "- 학습 때 배치 구성은 랜덤으로 하는것이 좋음\n",
    "- 테스트할 때는 굳이 랜덤할 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c20e04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler\n",
    "val_dataset = ClassificationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"test\",\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ad1f2",
   "metadata": {},
   "source": [
    "### 5. 모델 불러오기\n",
    "- pretrained_model_name을 beomi/kcbert-base로 지정했으므로 프리트레인을 마친 BERT로 kcbert-base를 사용\n",
    "- 모델을 초기화하는 코드에서 BertForSequenceClassification은 프리트레인을 마친 BERT 모델 위에 문서 분류용 태스크 모듈이 덧붙여진 형태의 모델 클래스이다.\n",
    "- 이 클래스는 허깅페이스에서 제공하는 transformers라이브러리에 포함돼있다.\n",
    "- 허깅페이스 모델 허브에 등록된 모델이라면 별다른 코드 수정없이 kcbert-base 이외에 다른 언어 모델을 사용할 수 있다.\n",
    "- 예를 들어 bert-base-uncased 모델은 구글이 공개한 다국어 BERT 모델인데 pretrained_model_name에 이 모델명을 입력하면 해당 모델을 쓸 수 있다. 허깅페이스에 등록된 모델 목록은 huggingface.co/models에서 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d14bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    num_labels=corpus.num_labels,\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    config=pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5ae34",
   "metadata": {},
   "source": [
    "### 6. 모델 학습시키기\n",
    "- 파이토치 라이트닝이 제공하는 LightningModule 클래스를 상속받아 태스크를 정의한다.\n",
    "- 태스크에는 모델과 옵티마이저, 학습과정 등이 정의돼 있다.\n",
    "- 다음 코드를 실행하면 문서 분류용 태스크를 정의할 수 있다.\n",
    "- model을 ClassificationTask에 주입한다.\n",
    "- ClassificationTask에는 옵티마이저, 러닝 레이트 스케줄러가 정의돼있는데 옵티마이저로는 아담, 러닝 레이트 스케줄러로는 ExponentialLR을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a73812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsnlp.nlpbook.classification import ClassificationTask\n",
    "task = ClassificationTask(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6df6f35",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "TPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mnlpbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\ratsnlp\\nlpbook\\trainer.py:17\u001b[0m, in \u001b[0;36mget_trainer\u001b[1;34m(args, return_trainer_only)\u001b[0m\n\u001b[0;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(ckpt_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m     11\u001b[0m     dirpath\u001b[38;5;241m=\u001b[39mckpt_path,\n\u001b[0;32m     12\u001b[0m     save_top_k\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msave_top_k,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{epoch}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{val_loss:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_dev_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_sanity_val_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_root_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# For GPU Setup\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# For TPU Setup\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtpu_cores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtpu_cores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_trainer_only:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:339\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:483\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m DataConnector(\u001b[38;5;28mself\u001b[39m, multiple_trainloader_mode)\n\u001b[1;32m--> 483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43mAcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_cores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mipus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mipus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_select_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_select_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m LoggerConnector(\u001b[38;5;28mself\u001b[39m, log_gpu_memory)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:196\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[1;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_accelerator()\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_parallel_devices_and_init_accelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_environment: ClusterEnvironment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_and_init_cluster_environment()\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:514\u001b[0m, in \u001b[0;36mAcceleratorConnector._set_parallel_devices_and_init_accelerator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    511\u001b[0m     available_accelerator \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    512\u001b[0m         acc_str \u001b[38;5;28;01mfor\u001b[39;00m acc_str \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_types \u001b[38;5;28;01mif\u001b[39;00m AcceleratorRegistry\u001b[38;5;241m.\u001b[39mget(acc_str)\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[0;32m    513\u001b[0m     ]\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m can not run on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m since the accelerator is not available. The following accelerator(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is available and can be passed into `accelerator` argument of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `Trainer`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_accelerator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_devices_flag_if_auto_passed()\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_devices_flag \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gpus \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gpus\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: TPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."
     ]
    }
   ],
   "source": [
    "trainer = nlpbook.get_trainer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec1d58f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTrainer\u001b[49m(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "Trainer(accelerator=\"tpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500db0c",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [NsmcCorpus, ClassificationDataset의 역할](https://ratsgo.github.io/nlpbook/docs/doc_cls/detail/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
